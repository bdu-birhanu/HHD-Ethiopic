{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXTxmO8YyjUIeQrsr8kxqx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdu-birhanu/HHD-Ethiopic/blob/main/train_HPopt_Attn_CTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranining an attention agumented CTC model\n"
      ],
      "metadata": {
        "id": "6LF-cJfWSb4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries:\n",
        "\n",
        "Begin by importing the necessary libraries and frameworks include TensorFlow, Keras, and  any other libraries as provided below."
      ],
      "metadata": {
        "id": "8_gFy_N7RWt9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1NZN09R6OjG",
        "outputId": "a47a8c46-8fe7-460e-bc14-b62c3c270ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Bidirectional, MaxPool2D\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Permute, Flatten, Masking, \\\n",
        "    GaussianNoise, Reshape, Lambda, TimeDistributed, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import RepeatVector, Multiply, Dot, Bidirectional,LSTM, Input, Dense, Activation,Lambda,Concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import cv2\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the training data\n",
        "\n",
        "Download the training data from Hugging Face. The dataset is in a format of .npy and it is compatible with this platform.\n"
      ],
      "metadata": {
        "id": "mM0A1nebR1Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Download the zip file from Hugging Face (when coping ht elink just replace \"blob\" by \"resolve\" so as to reference the file contents.)\n",
        "url=\"https://huggingface.co/datasets/OCR-Ethiopic/HHD-Ethiopic/resolve/main/train/train_numpy.zip\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the zip file\n",
        "zip_path = \"/content/train_numpy.zip\"\n",
        "with open(zip_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Extract the zip file\n",
        "extract_path = \"/content/train\"\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "num_class=307\n",
        "maxlen=46\n",
        "im_row=48\n",
        "im_col=368"
      ],
      "metadata": {
        "id": "qjriFoEV4PmC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    '''\n",
        "    '''\n",
        "    x_train_real= np.load('/content/train/train_numpy/x_train.npy',allow_pickle=True)\n",
        "    y_train_padded= np.load('/content/train/train_numpy/y_train.npy',allow_pickle=True)\n",
        "    \n",
        "\n",
        "    return x_train_real, y_train_padded \n",
        "\n"
      ],
      "metadata": {
        "id": "0D5_ywhf4F_F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize():\n",
        "    data_loaded=load_dataset()\n",
        "    x_train_real=data_loaded[0]\n",
        "    y_train_real = data_loaded[1]\n",
        "\n",
        "    resized_x_rand = np.zeros((len(x_train_real), im_row, im_col), dtype=np.uint8)\n",
        "\n",
        "    # loop over the input images\n",
        "    for i, image in enumerate(x_train_real):\n",
        "        # resize the image to 48 by 368 using padding\n",
        "        current_height, current_width = image.shape[:2]\n",
        "        aspect_ratio_current = current_width / current_height\n",
        "        aspect_ratio_target = im_col / im_row\n",
        "        if aspect_ratio_current != aspect_ratio_target:\n",
        "            if aspect_ratio_current > aspect_ratio_target:\n",
        "                new_height = int(current_width / aspect_ratio_target)\n",
        "                top_padding = (new_height - current_height) // 2\n",
        "                bottom_padding = new_height - current_height - top_padding\n",
        "                padded_image = cv2.copyMakeBorder(image, top_padding, bottom_padding, 0, 0, cv2.BORDER_CONSTANT, value=255)\n",
        "            else:\n",
        "                new_width = int(current_height * aspect_ratio_target)\n",
        "                left_padding = (new_width - current_width) // 2\n",
        "                right_padding = new_width - current_width - left_padding\n",
        "                padded_image = cv2.copyMakeBorder(image, 0, 0, 0, right_padding+left_padding, cv2.BORDER_CONSTANT, value=255)\n",
        "        else:\n",
        "            padded_image = image\n",
        "        resized_image = cv2.resize(padded_image, (im_col, im_row))\n",
        "\n",
        "        resized_x_rand[i] = resized_image\n",
        "\n",
        "    return resized_x_rand, y_train_real"
      ],
      "metadata": {
        "id": "hqMeNxpr5pqx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_train_val_test_data():\n",
        "    ''' \n",
        "    input: a 2D shape text-line image (h,w)\n",
        "    output:  returns 3D shape image format (h,w,1)\n",
        "\n",
        "    Plus this function randomly splits the training and validation set\n",
        "    This function also computes list of length for both training and validation images and GT\n",
        "      '''\n",
        "    #the function resize is excuted here\n",
        "    x_tr=resize()\n",
        "    x_train_real_resize=x_tr[0]\n",
        "    y_train_real = x_tr[1]\n",
        "  \n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train_real_resize, y_train_real, test_size=0.1)\n",
        "\n",
        "    # reshape the size of the image from 3D to 4D so as to make the input size is similar with it.\n",
        "    x_train_r = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)  # [samplesize,32,128,1]\n",
        "    x_val_r = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], 1)\n",
        "    y_train = y_train\n",
        "    y_val = y_val\n",
        "\n",
        "    nb_train = len(x_train_r)\n",
        "    nb_val = len(x_val_r)\n",
        "\n",
        "\n",
        "    x_train_len = np.array([len(x_train_r[i]) + 43 for i in range(nb_train)])\n",
        "    # the + 43 here is just to make  balanceed size ((2*46-1=91) then 48+43=91)of the image equal to the input of LSTMlayer\n",
        "    x_val_len = np.array([len(x_val_r[i]) + 43 for i in range(nb_val)])  \n",
        "    \n",
        "    y_train_len = np.array([len(y_train[i]) for i in range(nb_train)])\n",
        "    y_val_len = np.array([len(y_val[i]) for i in range(nb_val)])\n",
        "\n",
        "\n",
        "    return x_train_r, y_train, x_train_len, y_train_len, x_val, y_val, x_val_len, y_val_len\n",
        "'''\n",
        "all set of text images and GT\n",
        "'''\n",
        "train=preprocess_train_val_test_data()\n",
        "x_train=train[0]\n",
        "y_train=train[1]\n",
        "x_train_length=train[2]\n",
        "y_train_length=train[3]\n",
        "\n",
        "x_val=train[4]\n",
        "y_val=train[5]\n",
        "x_val_length=train[6]\n",
        "y_val_length=train[7]\n",
        "\n",
        "\n",
        "print(\"data_loading is compeletd\")\n",
        "print(\"===============================\")\n",
        "print(str(len(x_train))+ \" train image and \"+ str(len(y_train))+\" labels\")\n",
        "print(str(len(x_val))+ \"valid image and \"+ str(len(y_val))+ \"labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYeLTtJV50Ib",
        "outputId": "d771516b-8c9c-44f8-dfd5-a682458bf88b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_loading is compeletd\n",
            "===============================\n",
            "51636 train image and 51636 labels\n",
            "5738valid image and 5738labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition of the attention module\n"
      ],
      "metadata": {
        "id": "6LKpsskaSkK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_pro = Dot(axes = 1)\n",
        "concatenat = Concatenate(axis=-1)"
      ],
      "metadata": {
        "id": "oKpBvfNl6ki4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(blstm_2):\n",
        "    score = Dense(91, activation='tanh', name='attention_score_vec')(blstm_2)\n",
        "\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "\n",
        "   \n",
        "    context_vector = dot_pro([attention_weights,blstm_2])\n",
        "    \n",
        "\n",
        "    return  context_vector\n"
      ],
      "metadata": {
        "id": "Q4lQAoEl7WjS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A function for CTC"
      ],
      "metadata": {
        "id": "SqmLt64dS4eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_lambda_func(args):\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "metadata": {
        "id": "-r8eyzs47pMq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition of the CNN-LSTM-CTC"
      ],
      "metadata": {
        "id": "C9AagjosTN6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hist_ocr_model(config: dict, n_components: int = 5, verbose: bool = 0, num_class=num_class, img_row=im_row, img_col=im_col):\n",
        "    tf.keras.utils.set_random_seed(2)\n",
        "\n",
        "    default_config = {\n",
        "        \"rnn_size\": 128,\n",
        "        \"feature_map_1\": 64,\n",
        "        \"feature_map_2\": 128,\n",
        "        \"activation\": \"relu\",\n",
        "        \"batch_size\": 32,\n",
        "        \"epoch\": 10,\n",
        "        \"kernel\": 3,\n",
        "        \"dropout\": 0.25,\n",
        "\n",
        "    }\n",
        "    default_config.update(config)\n",
        "    '''\n",
        "    if you use the full datset you could increase the batch_size and epo\n",
        "    '''\n",
        "    \n",
        "    inputs_data = Input(shape=(img_row, img_col, 1))\n",
        "    conv_1 = Conv2D(default_config[\"feature_map_1\"], (default_config[\"kernel\"], default_config[\"kernel\"]),\n",
        "                    activation=default_config[\"activation\"], padding='same')(inputs_data)\n",
        "    \n",
        "    pool_1 = MaxPool2D(pool_size=(2, 1), strides=2)(conv_1)\n",
        "    conv_2 = Conv2D(default_config[\"feature_map_1\"], (default_config[\"kernel\"], default_config[\"kernel\"]),\n",
        "                    activation=default_config[\"activation\"], padding='same')(pool_1)\n",
        "    pool_2 = MaxPool2D(pool_size=(2, 2))(conv_2)  # we remove the strides here\n",
        "    conv_3 = Conv2D(default_config[\"feature_map_1\"], (default_config[\"kernel\"], default_config[\"kernel\"]),\n",
        "                    activation=default_config[\"activation\"], padding='same')(pool_2)\n",
        "    conv_4 = Conv2D(default_config[\"feature_map_1\"], (default_config[\"kernel\"], default_config[\"kernel\"]),\n",
        "                    activation=default_config[\"activation\"], padding='same')(conv_3)\n",
        "    \n",
        "    pool_3 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        "    \n",
        "    batch_norm_5 = BatchNormalization()(pool_3)\n",
        "    conv_5 = Conv2D(default_config[\"feature_map_1\"], (default_config[\"kernel\"], default_config[\"kernel\"]),\n",
        "                    activation=default_config[\"activation\"], padding='same')(batch_norm_5)\n",
        "\n",
        "    conv_6 = Conv2D(default_config[\"feature_map_1\"], (default_config[\"kernel\"], default_config[\"kernel\"]),\n",
        "                    activation=default_config[\"activation\"], padding='same')(conv_5)\n",
        "    batch_norm_6 = BatchNormalization()(conv_6)\n",
        "    # conv_7 = Conv2D(default_config[\"feature_map_1\"], (default_config[\"kernel\"], default_config[\"kernel\"]),\n",
        "    #                 activation=default_config[\"activation\"], padding='same')(batch_norm_6)\n",
        "    conv_7 = Conv2D(128, (2, 2), activation='relu')(batch_norm_6)\n",
        "    r = Reshape((int(conv_7.shape[2]), int(conv_7.shape[1]) * int(conv_7.shape[3])))(conv_7)\n",
        "\n",
        "    blstm_1 = Bidirectional(LSTM(default_config[\"rnn_size\"], return_sequences=True, dropout=default_config[\"dropout\"]))(r)\n",
        "    blstm_2 = Bidirectional(LSTM(default_config[\"rnn_size\"], return_sequences=True, dropout=default_config[\"dropout\"]))(blstm_1)\n",
        "\n",
        "\n",
        "    context = attention(blstm_2)\n",
        "    outputs = Dense(num_class + 1, activation='softmax')(context)\n",
        "\n",
        "    pred_model = Model(inputs_data, outputs)\n",
        "\n",
        "    labels = Input(name='the_labels', shape=[46], dtype='float32')  # 46 is the max size of text length\n",
        "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "    model = Model(inputs=[inputs_data, labels, input_length, label_length], outputs=loss_out)\n",
        "\n",
        "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam')\n",
        "\n",
        "    hist = model.fit(x=[x_train, y_train, x_train_length, y_train_length], y=np.zeros(len(y_train)),\n",
        "                     batch_size=default_config[\"batch_size\"], epochs=100,\n",
        "                     validation_data=([x_val, y_val, x_val_length, y_val_length], [np.zeros(len(y_val))]),\n",
        "                     verbose=1)\n",
        "\n",
        "    return model, pred_model, hist"
      ],
      "metadata": {
        "id": "ti6EPnxr7la8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the best hyperparameter values\n",
        "\n",
        "refer **[here](https://github.com/bdu-birhanu/Bayesian-Optimization-for-DNN-hyperparameter-tuning-)** to know more how you could select hyperparamnters' value with Bayesian Optimization"
      ],
      "metadata": {
        "id": "k52otaSUTVfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wget\n",
        "import wget\n",
        "import tensorflow as tf\n",
        "\n",
        "hyp_url = \"https://github.com/bdu-birhanu/HHD-Ethiopic/raw/main/src/all_code/best_hyp_train_attention.txt\"\n",
        "hyp_path = \"best_hyp_train_attention.txt\"\n",
        "# Download the model file\n",
        "wget.download(hyp_url, hyp_path)\n",
        "with open(hyp_path) as f:\n",
        "    data = f.read()\n",
        "best_config = eval(data)# toget dictionary from the string which is saved in the disk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J94PhScxBfdw",
        "outputId": "5fac0fb9-9d5b-42dd-b5b5-2378b127eaac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training and saving\n",
        "\n",
        "This model can be trained multiple times by adjusting the value of the \"rounds\" parameter below. For example, if we set it to 2 (as given below), the model will run for two rounds, resulting in two separate model trainings with different weight initialization. Once the training is completed (in this demo, we just stop the training at the 5th epoch!!), you can save the trained models  and use them for prediction. To learn how to utilize the model for prediction, please refer to the instructions provided [here](https://github.com/bdu-birhanu/HHD-Ethiopic/blob/main/Test_HPopt-Attn-CTC.ipynb)."
      ],
      "metadata": {
        "id": "hx-SA4ieUa4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/model/'\n",
        "rounds=2\n",
        "for i in range(rounds):\n",
        "    #model_train = hist_ocr_model()\n",
        "    best_model, best_pred_model, best_history = hist_ocr_model(best_config, n_components=5, verbose=1)\n",
        "    #model = model_train[0]\n",
        "    best_pred_model.save(path + f'model_belened_CTC_deephyper_{i}.hdf5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVTVxBkwAslZ",
        "outputId": "0a14b8b0-fcf2-49f3-bcb5-44741e52887d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1614/1614 [==============================] - 156s 90ms/step - loss: 86.2180 - val_loss: 70.9531\n",
            "Epoch 2/100\n",
            "1614/1614 [==============================] - 137s 85ms/step - loss: 62.2865 - val_loss: 61.6982\n",
            "Epoch 3/100\n",
            "1614/1614 [==============================] - 138s 85ms/step - loss: 61.0643 - val_loss: 60.9203\n",
            "Epoch 4/100\n",
            "1614/1614 [==============================] - 138s 86ms/step - loss: 60.0359 - val_loss: 59.4527\n",
            "Epoch 5/100\n",
            "1614/1614 [==============================] - 139s 86ms/step - loss: 58.9088 - val_loss: 58.3207\n",
            "Epoch 6/100\n",
            " 468/1614 [=======>......................] - ETA: 1:32 - loss: 57.9267"
          ]
        }
      ]
    }
  ]
}